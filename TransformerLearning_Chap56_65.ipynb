{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5Z6s2jJjqESuN7bnREbTA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingabLee/Transformers_book/blob/main/TransformerLearning_Chap56_65.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX4jpP6-ZVvz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    [\"What music do you like?\", \"I like Rock music.\", 1],\n",
        "    [\"What is your favorite food?\", \"I like sushi the best\", 1],\n",
        "    [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n",
        "    [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n",
        "    [\"Do you like watching TV show?\", \"Yeah, I often watch it in my spare time\", 1]\n",
        "]"
      ],
      "metadata": {
        "id": "c1nJDhD3ZhuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer\n",
        "from torch.optim import adamw\n",
        "from torch import nn\n",
        "\n",
        "# define class\n",
        "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
        "  # creator\n",
        "  def __init__(self, config, *args, **kwargs):\n",
        "    super().__init__(config)\n",
        "\n",
        "    #QA BERT Model\n",
        "    self.bert_model_1 = BertModel(config)\n",
        "\n",
        "    #AQ BERT Model\n",
        "    self.bert_model_2 = BertModel(config)\n",
        "\n",
        "    # linear function\n",
        "    self.cls = nn.Linear(2*self.config.hidden_size, 2)\n",
        "\n",
        "    # init weight\n",
        "    self.init_weights()\n",
        "\n",
        "  # forward\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids=None,\n",
        "      attention_mask=None,\n",
        "      token_type_ids=None,\n",
        "      position_ids=None,\n",
        "      head_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      next_sentence_label=None ):\n",
        "    # outputs list\n",
        "    outputs = []\n",
        "\n",
        "    #save input_ids first text\n",
        "    input_ids_1 = input_ids[0]\n",
        "\n",
        "    # save input_ids attention_mask\n",
        "    attention_mask_1 = attention_mask[0]\n",
        "\n",
        "    #save outputs result that  bert_model_1 result\n",
        "    outputs.append( self.bert_model_1(input_ids_1, attention_mask=attention_mask_1))\n",
        "\n",
        "    #save second text(input_ids)\n",
        "    input_ids_2 = input_ids[1]\n",
        "\n",
        "    #save second text attenton_mask\n",
        "    attention_mask_2 = attention_mask[1]\n",
        "\n",
        "    #save result of bert_model_2 input_ids_2\n",
        "    outputs.append(self.bert_model_2(input_ids_2, attention_mask=attention_mask_2))\n",
        "\n",
        "    # outputs에 쌓인 output의 두번째 요소(output[1])를 하나씩 추출하여\n",
        "    # torch.cat()으로 토치 텐서 형태로 병합\n",
        "    # 이를 통해 마지막 은닉층 임베딩 상태를 구함.\n",
        "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
        "\n",
        "    #self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출.\n",
        "    logits = self.cls(last_hidden_states)\n",
        "\n",
        "    # crossentropy loss\n",
        "    if next_sentence_label is not None:\n",
        "      # nn.CrossEntropyLoss() 입력 데이타의 마지막 인덱스는 계산에서 제외.\n",
        "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "      # logits.view(-1, 2)는 열이 2개 형태로 logits를 정렬\n",
        "      # next_sentence_label.view(-1)는 행이 하나인 형태로 정렬\n",
        "      next_sentence_loss = loss_fct(logits.view(-1,2), next_sentence_label.view(-1))\n",
        "      return next_sentence_loss, logits\n",
        "    else:\n",
        "      return logits"
      ],
      "metadata": {
        "id": "DLOdusD2bi-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# colab -> gpu, else -> cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model, config\n",
        "config = BertConfig()\n",
        "model = BertEnsembleForNextSentencePrediction(config)\n",
        "\n",
        "# send model to gpu or cpu\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# set learning rate\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# 절편과 가중치를 no_decay변수에 저장\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "# set optimize parameter group\n",
        "optimizer_grouped_parameters = [{\n",
        "    \"params\":[ p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "}]\n",
        "\n",
        "# optimizer\n",
        "optimizer = adamw.AdamW(optimizer_grouped_parameters,lr=learning_rate)"
      ],
      "metadata": {
        "id": "Gbx4IFh1Zc_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare_data function\n",
        "def prepare_data(dataset, qa=True):\n",
        "  # empty list\n",
        "  input_ids, attention_masks = [], []\n",
        "  labels = []\n",
        "\n",
        "  for point in dataset:\n",
        "    if qa is True:\n",
        "      # point에 있는 3개의 원소를 앞에서부터 q, a, _로 할당\n",
        "      q, a, _ = point\n",
        "    else :\n",
        "      a, q, _ = point\n",
        "\n",
        "    # q, a encode by tokenizer\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        q, # text1 encode\n",
        "        a,  # text2 encode\n",
        "        add_special_tokens = True,  #  speicial token[CLS], [SEP] add\n",
        "        max_length=128,\n",
        "        pad_to_max_length=True,   # padding to max_length\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt',  #pt -> pytorch\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # encoded_dict(\"input_ids\")를 컨테이너 변수 input_ids에 순서대로 저장\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "\n",
        "    # encoded_dict(\"attention_mask\")를 attention_masks에 순서대로 저장\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "\n",
        "    # point의 마직막 (세번째) 원소(레이블)을 labels에 순서대로 저장.\n",
        "    labels.append(point[-1])\n",
        "\n",
        "    # end for loop\n",
        "\n",
        "  # input_ids 첫번째 축(dim=0), 세로방향으로 병합\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "  #attention_mask도 첫번째 축(dim=0), 새로방향으로 병합\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "  # return input_ids, attention_masks, labels\n",
        "  return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "EC0-bz6fjTCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
        "\n",
        "# QADataset class\n",
        "class QADataset(Dataset):\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self, input_ids, attention_masks, labels=None):\n",
        "    self.input_ids = np.array(input_ids)\n",
        "    self.attention_masks = np.array(attention_masks)\n",
        "    # torch.long은 정수타입을 의미\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.attention_masks[index], self.labels[index]# if self.labels is not None else None\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_ids.shape[0]"
      ],
      "metadata": {
        "id": "eszN-vuymrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset을 prepare_data에 투입하여 결과를 각기\n",
        "# input_ids_qa, attention_mask_qa, label_qa에 저장\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "\n",
        "# 위의 결과물을 QADataset클래스에 투입\n",
        "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "# 맨 윗줄 코드와 동일하나 이번에는 prepare_data에 qa플래그 값이 false일때 적용\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "\n",
        "# 위의 결과물을 QADataset에 클래스에 투입\n",
        "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "# train_dataset_qa를 dataloader로 처리\n",
        "dataloader_qa = DataLoader(dataset=train_dataset_qa,\n",
        "                           batch_size=5, sampler=SequentialSampler(train_dataset_qa))\n",
        "\n",
        "# train_dataset_aq dataloader 처리\n",
        "dataloader_aq = DataLoader(dataset=train_dataset_aq,\n",
        "                           batch_size=5, sampler=SequentialSampler(train_dataset_aq))"
      ],
      "metadata": {
        "id": "3QYfWYtSpDE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run time 8minute\n",
        "# epoch = 30\n",
        "epochs = 30\n",
        "\n",
        "# for loop epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # dataloader_qa, dataloader_aq 쌍을 동시 반복 루프에서 처리\n",
        "  # enumerate 및 zip으로 두 데이터 쌍을 묶고 반복가능한 순서 부여\n",
        "  for step,combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "    # enumerate로 묶인 데이터쌍을 순서대로 batch_1,batch_2로 저장\n",
        "    batch_1, batch_2 = combined_batch\n",
        "\n",
        "    #모델을 학습모드로 전환\n",
        "    model.train()\n",
        "\n",
        "    # 가능한 경우 batch_1과 batch_2의 데이터를 GPU에 전달하고\n",
        "    # 그렇지 않은 경우 cpu로 전달\n",
        "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "    #모델에 투입할 변수 inputs의 내용입력\n",
        "    inputs = {\n",
        "        \"input_ids\" : [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [ batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\" : batch_1[2]\n",
        "    }\n",
        "\n",
        "    # 모델에 inputs를 **kwargs 형식(**inputs)으로 투입\n",
        "    # 딕셔너리 타입인 inputs의 키와 값 모두 입력\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # 모델의 결과물인 outputs는 tuple타입으로  출력\n",
        "    # 그중 첫번째 요소, 즉 outputs[0]을 변수 loss에 저장\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # error backward\n",
        "    loss.backward()\n",
        "\n",
        "    # print epoch, loss\n",
        "    print(f\"epoch:{epoch}, loss{loss}\")\n",
        "\n",
        "    #update weight\n",
        "    optimizer.step()\n",
        "\n",
        "    # gradient init for next epoch\n",
        "    model.zero_grad()\n"
      ],
      "metadata": {
        "id": "OWEdYAbNseaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset prepare_data function\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "\n",
        "#QADataset class\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "#dataset prepare_data function with qa flags is False\n",
        "input_ids_aq, attention_masks_as, labels_aq = prepare_data(dataset, qa=False)\n",
        "\n",
        "#QADataset class\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "# test_dataset_qa -> DataLoader\n",
        "dataloader_qa = DataLoader(dataset=test_dataset_qa,\n",
        "                           batch_size=16, sampler=SequentialSampler(test_dataset_qa))\n",
        "\n",
        "# test_dataset_aq -> DalataLoader\n",
        "dataloader_aq = DataLoader(dataset=test_dataset_aq, batch_size=16,\n",
        "                           sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "# container list for result\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "# dataloader_qa, dataloader_aq를 동시에 반복루프 작업실시\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)) :\n",
        "  # model eval\n",
        "  model.eval()\n",
        "\n",
        "  # batch1, batch2 <- enumerated zip\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  # 가능한 경우 batch_1, batch_2를 gpu로 전달. 아니면 cpu전달\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  # no grade. evalution only need forward propagation.\n",
        "  # prevent auto gradient calc\n",
        "  with torch.no_grad():\n",
        "    # inputs -> model\n",
        "    inputs = {\n",
        "        \"input_ids\":[batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\":[batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\":batch_1[2]\n",
        "    }\n",
        "\n",
        "    #inputs -> model (**kwargs)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # temp_eval_loss <- outputs first item\n",
        "    # logits <- outputs second item\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "    # logit -> cpu -> numpy\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # logits에 담긴 로짓값을 axis=1, 즉 가로방향으로 최대값 인덱스 축출\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "\n",
        "    #inputs['next_sentence_label'] -> cpu -> numpy\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "\n",
        "    # with torch.no_grad end\n",
        "\n",
        "  # outputs, label_ids를 각각 container list 에 순서대로 저장\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "# print final result\n",
        "print(complete_outputs,complete_label_ids)\n",
        "\n",
        "# [1,1,0,0,1] [1,1,0,0,1] 첫번째는 예측된 결과, 두번째는 label 결과\n",
        "\n"
      ],
      "metadata": {
        "id": "wZM32F6zzPiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전과 동일 내용이므로 주석 설명생략\n",
        "dataset = [[\"What music to you like?\",\"I like Rock music.\", 1]]\n",
        "\n",
        "input_ids_qa,attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa = DataLoader(dataset=test_dataset_qa,\n",
        "                           batch_size=16, sampler=SequentialSampler(test_dataset_qa))\n",
        "\n",
        "dataloader_aq = DataLoader(dataset=test_dataset_aq,\n",
        "                           batch_size=16, sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "  model.eval()\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    inputs = {\n",
        "        \"input_ids\":[batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\":[batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\":batch_1[2]\n",
        "    }\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "print(complete_outputs, complete_label_ids)"
      ],
      "metadata": {
        "id": "_DMbM6wdilCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyjPLW4emiz2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}