{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5Ixb8SYtZP2pyxZ8rJSol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingabLee/Transformers_book/blob/main/TransformerLearning_Chap56_65.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX4jpP6-ZVvz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    [\"What music do you like?\", \"I like Rock music.\", 1],\n",
        "    [\"What is your favorite food?\", \"I like sushi the best\", 1],\n",
        "    [\"What is your favorite color?\", \"I'm going to be a doctor\", 0],\n",
        "    [\"What is your favorite song?\", \"Tokyo olympic game in 2020 was postponed\", 0],\n",
        "    [\"Do you like watching TV show?\", \"Yeah, I often watch it in my spare time\", 1]\n",
        "]"
      ],
      "metadata": {
        "id": "c1nJDhD3ZhuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer\n",
        "from torch.optim import adamw\n",
        "from torch import nn\n",
        "\n",
        "# define class\n",
        "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
        "  # creator\n",
        "  def __init__(self, config, *args, **kwargs):\n",
        "    super().__init__(config)\n",
        "\n",
        "    #QA BERT Model\n",
        "    self.bert_model_1 = BertModel(config)\n",
        "\n",
        "    #AQ BERT Model\n",
        "    self.bert_model_2 = BertModel(config)\n",
        "\n",
        "    # linear function\n",
        "    self.cls = nn.Linear(2*self.config.hidden_size, 2)\n",
        "\n",
        "    # init weight\n",
        "    self.init_weights()\n",
        "\n",
        "  # forward\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids=None,\n",
        "      attention_mask=None,\n",
        "      token_type_ids=None,\n",
        "      position_ids=None,\n",
        "      head_mask=None,\n",
        "      inputs_embeds=None,\n",
        "      next_sentence_label=None ):\n",
        "    # outputs list\n",
        "    outputs = []\n",
        "\n",
        "    #save input_ids first text\n",
        "    input_ids_1 = input_ids[0]\n",
        "\n",
        "    # save input_ids attention_mask\n",
        "    attention_mask_1 = attention_mask[0]\n",
        "\n",
        "    #save outputs result that  bert_model_1 result\n",
        "    outputs.append( self.bert_model_1(input_ids_1, attention_mask=attention_mask_1))\n",
        "\n",
        "    #save second text(input_ids)\n",
        "    input_ids_2 = input_ids[1]\n",
        "\n",
        "    #save second text attenton_mask\n",
        "    attention_mask_2 = attention_mask[1]\n",
        "\n",
        "    #save result of bert_model_2 input_ids_2\n",
        "    outputs.append(self.bert_model_2(input_ids_2, attention_mask=attention_mask_2))\n",
        "\n",
        "    # outputs에 쌓인 output의 두번째 요소(output[1])를 하나씩 추출하여\n",
        "    # torch.cat()으로 토치 텐서 형태로 병합\n",
        "    # 이를 통해 마지막 은닉층 임베딩 상태를 구함.\n",
        "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
        "\n",
        "    #self.cls 선형함수에 마지막 은닉층 임베딩 상태를 투입하여 로짓 추출.\n",
        "    logits = self.cls(last_hidden_states)\n",
        "\n",
        "    # crossentropy loss\n",
        "    if next_sentence_label is not None:\n",
        "      # nn.CrossEntropyLoss() 입력 데이타의 마지막 인덱스는 계산에서 제외.\n",
        "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "      # logits.view(-1, 2)는 열이 2개 형태로 logits를 정렬\n",
        "      # next_sentence_label.view(-1)는 행이 하나인 형태로 정렬\n",
        "      next_sentence_loss = loss_fct(logits.view(-1,2), next_sentence_label.view(-1))\n",
        "      return next_sentence_loss, logits\n",
        "    else:\n",
        "      return logits"
      ],
      "metadata": {
        "id": "DLOdusD2bi-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# colab -> gpu, else -> cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model, config\n",
        "config = BertConfig()\n",
        "model = BertEnsembleForNextSentencePrediction(config)\n",
        "\n",
        "# send model to gpu or cpu\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# set learning rate\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# 절편과 가중치를 no_decay변수에 저장\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "# set optimize parameter group\n",
        "optimizer_grouped_parameters = [{\n",
        "    \"params\":[ p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "}]\n",
        "\n",
        "# optimizer\n",
        "optimizer = adamw.AdamW(optimizer_grouped_parameters,lr=learning_rate)"
      ],
      "metadata": {
        "id": "Gbx4IFh1Zc_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare_data function\n",
        "def prepare_data(dataset, qa=True):\n",
        "  # empty list\n",
        "  input_ids, attention_masks = [], []\n",
        "  labels = []\n",
        "\n",
        "  for point in dataset:\n",
        "    if qa is True:\n",
        "      # point에 있는 3개의 원소를 앞에서부터 q, a, _로 할당\n",
        "      q, a, _ = point\n",
        "    else :\n",
        "      a, q, _ = point\n",
        "\n",
        "    # q, a encode by tokenizer\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        q, # text1 encode\n",
        "        a,  # text2 encode\n",
        "        add_special_tokens = True,  #  speicial token[CLS], [SEP] add\n",
        "        max_length=128,\n",
        "        pad_to_max_length=True,   # padding to max_length\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt',  #pt -> pytorch\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # encoded_dict(\"input_ids\")를 컨테이너 변수 input_ids에 순서대로 저장\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "\n",
        "    # encoded_dict(\"attention_mask\")를 attention_masks에 순서대로 저장\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "\n",
        "    # point의 마직막 (세번째) 원소(레이블)을 labels에 순서대로 저장.\n",
        "    labels.append(point[-1])\n",
        "\n",
        "    # end for loop\n",
        "\n",
        "  # input_ids 첫번째 축(dim=0), 세로방향으로 병합\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "  #attention_mask도 첫번째 축(dim=0), 새로방향으로 병합\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "  # return input_ids, attention_masks, labels\n",
        "  return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "EC0-bz6fjTCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler\n",
        "\n",
        "# QADataset class\n",
        "class QADataset(Dataset):\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self, input_ids, attention_masks, labels=None):\n",
        "    self.input_ids = np.array(input_ids)\n",
        "    self.attention_masks = np.array(attention_masks)\n",
        "    # torch.long은 정수타입을 의미\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long) if labels is not None else None\n",
        "\n",
        "  def __get_item__(self, index):\n",
        "    return self.input_ids[index], self.attention_mask[index], self.labels[index] if self.labels is not None else None\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_ids.shape[0]"
      ],
      "metadata": {
        "id": "eszN-vuymrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QYfWYtSpDE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}