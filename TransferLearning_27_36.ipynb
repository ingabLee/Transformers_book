{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRfq836D+P5j3V3FjI2j9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingabLee/Transformers_book/blob/main/TransferLearning_27_36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrR-Kow0ZkiC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "\n",
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display image by PIL(pillow)\n",
        "from PIL import Image\n",
        "import requests\n",
        "ulr = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(ulr, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "eMsYvuMYzo9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "j6UEEugt0kqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "F39i0gum-Fw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input text list\n",
        "candidates = [\"three cats lying on the couch\",\n",
        "              \"a photo of a cat\",\n",
        "              \"a photo of a dog\",\n",
        "              \"a lion\",\n",
        "              \"two cats lying on the couch\"]\n",
        "\n",
        "# CLIPProcessor에 text및 이미지를 입력하여 인코딩\n",
        "inputs = processor(text=candidates, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "#out inputs\n",
        "inputs"
      ],
      "metadata": {
        "id": "2AJWtyoS-SfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display encoded image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(inputs['pixel_values'][0][0]);"
      ],
      "metadata": {
        "id": "umOum6RbpjCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs['pixel_values'] shape\n",
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "id": "2k9WQvpXr0MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['input_ids'][0]"
      ],
      "metadata": {
        "id": "CglwzeAWsbvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.tokenizer.decode(inputs['input_ids'][0])"
      ],
      "metadata": {
        "id": "H7vu5rjKvEX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change eval mode\n",
        "model.eval()\n",
        "\n",
        "# **inputs에서 **표시는 key와 key value으로 이루어진 input변수를\n",
        "# 모델에 입력할때 키와 값 모두 입력하는 용도임.\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# display output keys\n",
        "outputs.keys()"
      ],
      "metadata": {
        "id": "fYxt41uXvQDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_per_image  = outputs.logits_per_image\n",
        "print(logits_per_image)"
      ],
      "metadata": {
        "id": "yTpjJ85awK4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits_per_image에 담긴 값을 입력값(dim=1) softmax\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "import torch\n",
        "\n",
        "# prob에서 인덱스 최대값를 argmax로 찾고 item()통해 숫자로 출력\n",
        "# 그결과가 candiates의 인덱스 값이 됨.\n",
        "print(candidates[torch.argmax(probs).item()])"
      ],
      "metadata": {
        "id": "2-7AlSbh1DRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpXz9eQ741-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}