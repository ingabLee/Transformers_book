{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmzAYhQ4KtlIsErUb6sbij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingabLee/Transformers_book/blob/main/Transformer_17_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m61y412SZUXv"
      },
      "outputs": [],
      "source": [
        "#!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "#!pip3 install torch torchaudio torchvision torchtext==0.17.0 torchdata\n",
        "#!pip3 install portalocker\n",
        "#!pip3 install accelerate\n",
        "!pip3 install transformers\n",
        "!pip3 install sentencepiece\n",
        "\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# load model and tokenzier\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing encoding( single input text )\n",
        "input = tokenizer.encode(\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\n",
        "                         return_tensors=\"pt\")\n",
        "\n",
        "# 첫번째 인코딩 결과(결과적으로 하나의 문장)를 확인\n",
        "print(input[0])\n",
        "\n",
        "# 첫번째 인코딩결과를 디코딩\n",
        "print(tokenizer.decode(input[0]))"
      ],
      "metadata": {
        "id": "lAy4s3N9aI-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding by tokenizing( multiple input text)\n",
        "tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
        "\n",
        "input = tokenizer.batch_encode_plus([\n",
        "    \"I evaluated the performance of GPT-Neo developed by OpenAI.\",\n",
        "    \"I evaluated the performance of GPT develooped by OpenAI.\"],\n",
        "                                    padding=True,\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "# check encoding result\n",
        "print(input['input_ids'])\n",
        "\n",
        "#decoding\n",
        "print([tokenizer.decode(input['input_ids'][i]) for i in range(len(input['input_ids']))])"
      ],
      "metadata": {
        "id": "wev5-TO_cNDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing\n",
        "input = tokenizer.batch_encode_plus([\"I evaluated the performance of GPT2 devloped by OpenAI.\",\n",
        "                                     \"Vaccine for new cronavirus in UK\",\n",
        "                                     \"3.1415926536\"],\n",
        "                                     max_length=5, truncation=True,\n",
        "                                    padding=True, return_tensors=\"pt\")\n",
        "# check encode result.\n",
        "input['input_ids']\n",
        "\n",
        "# 인코딩 결과를 model.generate()에 투입.\n",
        "generated = model.generate(input['input_ids'])\n",
        "\n",
        "# check model.generate result\n",
        "len(generated)"
      ],
      "metadata": {
        "id": "pQvVU_8bdrJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deconding\n",
        "generated_text = tokenizer.batch_decode(generated)\n",
        "\n",
        "# enumerate()는 변수 generated_text에 있는 키값에 0,1,2... 로 시작하는 숫자 키값을 부여\n",
        "# f문자열 안의 \\n은 출력시 한줄을 띄우기를 의미\n",
        "for i, sentence in enumerate(generated_text):\n",
        "  print(f'No.{i+1}')\n",
        "  print(f'{sentence}\\n')"
      ],
      "metadata": {
        "id": "1IQXG9zlKzH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model, tokenizer\n",
        "from transformers import AutoTokenizer,AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "model = AutoModelWithLMHead.from_pretrained('distilgpt2')"
      ],
      "metadata": {
        "id": "zg2MoclLLnG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing\n",
        "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
        "\n",
        "# max-length 12\n",
        "greedy_output = model.generate(input_ids, max_length=12)\n",
        "\n",
        "# - 표시를 100번 실행하여 점선으로 구성된 줄 만들기\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# decoding result of encoding\n",
        "print(tokenizer.decode(greedy_output[0], skip_speical_token=True))"
      ],
      "metadata": {
        "id": "fobaeQ54ODUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
      ],
      "metadata": {
        "id": "q3pFqRWjFR5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing\n",
        "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
        "\n",
        "# max length = 30\n",
        "greedy_output = model.generate(input_ids, max_length=30)\n",
        "\n",
        "# decoding\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_speical_tokens=True))"
      ],
      "metadata": {
        "id": "OWFJZfVoGVuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try another text\n",
        "input_ids = tokenizer.encode(\"Covid19 delta is spreading\", return_tensors='pt')\n",
        "\n",
        "# max length = 50\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# decoding\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_speical_tokens=True))"
      ],
      "metadata": {
        "id": "CjSER9AwH0W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFsG_CcsIgpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}