{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxMo8OFXmhs2b5B39bSgbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ingabLee/Transformers_book/blob/main/Transformer_09_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4oP6-RDlPG"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers==4.30.0\n",
        "#!pip install torchtext==0.15.2\n",
        "#!pip install portalocker==2.7.0\n",
        "#!pip install accelerate -U\n",
        "#!pip install torch==2.6.0\n",
        "\n",
        "#!python --version\n",
        "\n",
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchaudio torchvision torchtext==0.17.0 torchdata\n",
        "!pip3 install portalocker\n",
        "!pip3 install accelerate\n",
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "\n",
        "a = np.array([0,0,1])\n",
        "b = torch.Tensor(a)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "c = b.tolist()\n",
        "print(c)\n"
      ],
      "metadata": {
        "id": "oDvwiLHHoAXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "test_iter = IMDB(split='test')\n",
        "\n",
        "# 출력 결과를 고정하기 위해\n",
        "import random\n",
        "random.seed(6)\n",
        "\n",
        "# train_iter를 리스트 타입으로 변경\n",
        "train_list = list(train_iter)\n",
        "test_list = list(test_iter)\n",
        "\n",
        "# 각기 1000개씩 random sampling\n",
        "train_list_small = random.sample(train_list, 1000)\n",
        "test_list_small = random.sample(test_list, 1000)\n",
        "\n",
        "# 각 변수에 담긴 인덱스 0의 내용 화면 출력\n",
        "print(train_list_small[0])\n",
        "print(test_list_small[0])"
      ],
      "metadata": {
        "id": "oXvDkgeoD6A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text, train_label 컨테이너 생성\n",
        "# 아래 반복문에서 생성된 결과를 담는다.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "#for 반복문\n",
        "# train_list_small에 담긴 튜플쌍 원소를 변수명 label, test부여하여 순서대로 축출\n",
        "for label, text in train_list_small:\n",
        "  # IMDB 데이터의 기존 레이블 2를 1로 변경, 기존 레이블 1을 0으로 변경.\n",
        "  train_labels.append(1 if label == 2 else 0)\n",
        "  train_texts.append(text)\n",
        "\n",
        "# test_list_small도 동일하게 처리\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for label, text in test_list_small:\n",
        "  test_labels.append(1 if label==2 else 0)\n",
        "  test_texts.append(text)\n",
        "\n",
        "# 각 변수에 담긴 첫번째 원소 출력\n",
        "print(train_texts[0])\n",
        "print(train_labels[0])\n",
        "print(test_texts[0])\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "id": "AhqvxIDUD-dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2,random_state=3)\n",
        "\n",
        "print(len(train_texts))\n",
        "print(len(train_labels))\n",
        "print(len(val_texts))\n",
        "print(len(val_labels))"
      ],
      "metadata": {
        "id": "HXl9XeuDEMeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distilbert-base-uncased 모델에서 tokenizer 불러오기\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "5ujSY8MyER3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer execute\n",
        "# truncation=True -> 모델의 default max_length를 넘는 입력부분은 더이상 받지않고 절단한다는 의미\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# 0번째 입력문의 5번째 토큰까지 input_ids출력\n",
        "print(train_encodings[\"input_ids\"][0][:5])\n",
        "\n",
        "#위의 결과를 다시 디코딩해서 출력\n",
        "print(tokenizer.decode(train_encodings[\"input_ids\"][0][:5]))"
      ],
      "metadata": {
        "id": "NiBBfe9yEWx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Dataset 클래스를 상속하는 IMDB class정의\n",
        "class IMdbDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  # contructor\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  #\n",
        "  def __getitem__(self, idx):\n",
        "    # self.encoding에 담긴 키(key)와 키값(value)를 items()로 축출\n",
        "    # 이 값을 key와 val 변수에 담아 새로운 키와 키값(torch.tensor(val[idx]))을 갖는 딕셔너리 생성\n",
        "\n",
        "    # 딕셔너리{\"key1 : value1\", ...} 형태를 가지는 데이터 구조.\n",
        "    # val[idx]에 담긴 데이터를 torch.tensor()을 통해 텐셔화.\n",
        "    item = {key:torch.tensor(val[idx]) for key, val in self.encodings.items() }\n",
        "\n",
        "    # tensor화\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  #\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "train_dataset = IMdbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMdbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMdbDataset(test_encodings, test_labels)\n",
        "\n",
        "for i in train_dataset:\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "id": "PU2YWGzmEi9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "# load distilbert model.  (need transformers==4.30.0)\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model"
      ],
      "metadata": {
        "id": "muR1kUQAEp9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./result',    # 출력 폴더 설정\n",
        "    num_train_epochs=8,       # training epochs count\n",
        "    per_device_train_batch_size=16, # train mini-batch size\n",
        "    per_device_eval_batch_size=64,  # eval min-batch size\n",
        "    warmup_steps=500,   # learning-rate scheduling warmup step\n",
        "    weight_decay=0.01,  # 가중치 감소 강도\n",
        "    logging_dir='./logs',   #로그 폴더 경로\n",
        "    logging_steps=10\n",
        ")"
      ],
      "metadata": {
        "id": "eTuHO-GXKn7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "\n",
        "# gpu사용이 가능한 경우 tensor타입를 gpu로 전달\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Q64gJIABQWhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tunning이전에 세입력 문장 극성 판별\n",
        "# tokenizing\n",
        "input_tokens = tokenizer([\"I feel fantasic\",\n",
        "                          \"My life is going something wrong.\",\n",
        "                          \"I have not figured out what the chosen title has to do with the movie.\"],\n",
        "                         truncation=True, padding=True)\n",
        "\n",
        "# input text tokenizing result에 담긴 input_ids를 모델에 투입\n",
        "# 그리고 모델 출력 결과를 gpu로 전송하며 값은 변수 outputs에 저장\n",
        "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
        "\n",
        "# create label dictionary\n",
        "label_dict = {0:'positvie', 1:'negative'}\n",
        "print(outputs['logits'])\n",
        "\n",
        "\n",
        "# outputs변수에 담긴 logits값을 행단위, 즉, 입력문장 단위로 가장 큰값의 위치(인덱스) 추출\n",
        "# 결과값(인덱스)을 cpu로 넘기고 넘파이 타입으로 변경후, 인덱스에 매칭되는 레이블 출력\n",
        "\n",
        "#print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])\n",
        "v = torch.argmax(outputs['logits'], axis=1)\n",
        "print(v)\n",
        "#print(v.cpu().numpy())\n",
        "#print(torch.__version__)\n",
        "\n",
        "w = v.tolist()\n",
        "print([label_dict[i] for i in w])"
      ],
      "metadata": {
        "id": "TWtIirmaSbT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maybe 5minute\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,      # pretrain model instance\n",
        "    args = training_args, # transformer.Arguments define hyper parameter\n",
        "    train_dataset = train_dataset,  # train data set\n",
        "    eval_dataset = val_dataset    # evaluation data set\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yY7iv1rJUmIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after fine-tunning. then try again three text\n",
        "\n",
        "input_tokens = tokenizer([\"I feel fantastic\",\n",
        "                          \"My life is going something wrong.\",\n",
        "                          \"I have not figured out what the chosen title has to do with the movie.\"],\n",
        "                         truncation=True, padding=True)\n",
        "\n",
        "# input text tokenizing result에 담긴 input_ids를 모델에 투입\n",
        "# 그리고 모델 출력 결과를 gpu로 전송하며 값은 변수 outputs에 저장\n",
        "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
        "\n",
        "# create label dictionary\n",
        "label_dict = {1:'positvie', 0:'negative'}\n",
        "print(outputs['logits'])\n",
        "\n",
        "\n",
        "# outputs변수에 담긴 logits값을 행단위, 즉, 입력문장 단위로 가장 큰값의 위치(인덱스) 추출\n",
        "# 결과값(인덱스)을 cpu로 넘기고 넘파이 타입으로 변경후, 인덱스에 매칭되는 레이블 출력\n",
        "\n",
        "#print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])\n",
        "v = torch.argmax(outputs['logits'], axis=1).tolist()\n",
        "print([label_dict[i] for i in v])"
      ],
      "metadata": {
        "id": "_KWJAHbGcoVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make function\n",
        "def test_inference(model, tokenizer):\n",
        "  input_tokens = tokenizer([\"I feel fantastic\",\n",
        "                          \"My life is going something wrong.\",\n",
        "                          \"I have not figured out what the chosen title has to do with the movie.\"],\n",
        "                         truncation=True, padding=True)\n",
        "\n",
        "  # input text tokenizing result에 담긴 input_ids를 모델에 투입\n",
        "  # 그리고 모델 출력 결과를 gpu로 전송하며 값은 변수 outputs에 저장\n",
        "  outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
        "\n",
        "  # create label dictionary\n",
        "  label_dict = {1:'positvie', 0:'negative'}\n",
        "\n",
        "  # outputs변수에 담긴 logits값을 행단위, 즉, 입력문장 단위로 가장 큰값의 위치(인덱스) 추출\n",
        "  # 결과값(인덱스)을 cpu로 넘기고 넘파이 타입으로 변경후, 인덱스에 매칭되는 레이블 출력\n",
        "\n",
        "  v = torch.argmax(outputs['logits'], axis=1).tolist()\n",
        "  return [label_dict[i] for i in v]"
      ],
      "metadata": {
        "id": "CrEoEBOQzGeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maybe 5minute take time\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "# 1) load pretrain model and tokenizer\n",
        "# add to code ( output of model \"to(device)\" ) 가능한 경우 결과를 gpu에 전달\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(device)\n",
        "\n",
        "#print result of before fine tunning\n",
        "print(test_inference(model, tokenizer))\n",
        "\n",
        "# 2) Dataloader instance\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 3) define optimize\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# 4) 모델을 학습(train)모드로 전환\n",
        "# 학습모드 전환은 dropout, batch-normalization에 영향을 미침\n",
        "model.train()\n",
        "\n",
        "losses=[]\n",
        "\n",
        "# 5) for loop epochs count\n",
        "for epoch in range(8):\n",
        "  print(f'epoch:{epoch}')\n",
        "  for batch in train_loader:\n",
        "    # 6) init optimize function gradient\n",
        "    optim.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    # 7) inference model\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    # 8) calc loss\n",
        "    loss = outputs[0]\n",
        "    losses.append(loss)\n",
        "\n",
        "    # 9) backward error\n",
        "    loss.backward()\n",
        "\n",
        "    # 10) update weight\n",
        "    optim.step()\n",
        "\n",
        "# change mode evaluate model\n",
        "model.eval()\n",
        "\n",
        "# run test_inference in eval mode.\n",
        "print(test_inference(model, tokenizer))"
      ],
      "metadata": {
        "id": "SZzD245s1Crp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_inference(model, tokenizer))\n",
        "print(losses)\n",
        "type(losses)"
      ],
      "metadata": {
        "id": "X5OJ6nLr4bcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch에서 item은 tensor값을 python number로 추출하여 cpu에 전달\n",
        "new_losses = [i.item() for i in losses]\n",
        "\n",
        "# display first 5 items\n",
        "new_losses[:5]\n",
        "type(new_losses)"
      ],
      "metadata": {
        "id": "371KlQxR8zxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_losses[-5:]"
      ],
      "metadata": {
        "id": "N5HmXrME9xRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(new_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ClUNhIZ1-Lew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PwDJMWH-Swn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}